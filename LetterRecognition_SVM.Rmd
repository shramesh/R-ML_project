---
title: "Letter Recognition using Support-Vector Machine (SVM)"
author: "ShRamesh"
date: "5/30/2021"
output: html_document
---

### Library
```{r include=FALSE}
if (!require(pacman)) intall.package("pacman")

p_load(pacman,
       tidyverse,
       caret,
       magrittr,
       class, #knn
       ggrepel,
       kernlab, #svm
       nnet,
       C50, #decision trees
       randomForest,
       gmodels,
       usemodels, # use_xgboost
       psych, #pca
       xgboost,
       patchwork,
       naniar) #for missing data analysis


theme_set(theme_minimal())
```

### Load dataset

The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15. 
Source: https://archive.ics.uci.edu/ml/datasets/letter+recognition
```{r}
data <-read_csv("Data/letter-recognition.csv")

str(data) # look at the structure of the data

data<-data %>% 
  mutate(letter=as.factor(letter)) #set the letters as factors
```

```{r}
# To view the data
data %>% 
    glimpse()

# proportion of data on each letter
data %>% 
    group_by(letter) %>% 
    summarise(count=n(),prop=count*100/nrow(data)) #%>% 

```

### Split into training & test data sets
```{r}
set.seed(100)

train_set = sample(1:nrow(data), size=floor(0.7*nrow(data))) #70% of the data is keept as training set

data_train= data[train_set,]

data_test= data[-train_set,]

```

### Proportion of letters after test-train split
```{r}

#Proportion of letters in train and test set
data_train %>% group_by(letter) %>% summarise(n=n(),prop=n*100/nrow(data_train))

data_test %>% group_by(letter) %>% summarise(n=n(),prop=n*100/nrow(data_test))

```

### SVM Model with Vanila dot kernel
```{r}
#vanilladot Linear kernel function
letter_classifier <- ksvm(letter ~ ., data = data_train,
                          kernel = "vanilladot")
```

#### Prediction and Confusion matrix
```{r}
# predictions on testing dataset
letter_predictions <- predict(letter_classifier, data_test)
cm1<-confusionMatrix(data_test$letter,letter_predictions) #Accuracy : 0.8453
cm1$overall[1]
#cm1[["byClass"]][,c('F1',"Recall","Precision")]
```


### SVM model with rbfdot kernel
```{r}
#Gaussian radial basis function rbfdot
letter_classifier_rbf <- ksvm(letter ~ ., data = data_train,
                          kernel = "rbfdot")

```

#### Predictions and confusion matrix
```{r}
letter_predictions_rbf <- predict(letter_classifier_rbf, data_test)
confusionMatrix(data_test$letter,letter_predictions_rbf) #Accuracy : 0.9268
```
It looks like the rbfdot model works better than the linear vanilladot model. So, keeping the model with rbf model, try to update parameters to increase the accuracy of the model.

### Hyper Parameter Tuning with rbf model
##### Sigma sets the inverse kernel width for the Radial Basis kernel function "rbfdot"
Cross sets a k-fold cross validation on the training data 
#### 1. Set sigma=0.9

```{r}
#Gaussian radial basis function rbfdot
letter_classifier_rbf2 <- ksvm(letter ~ ., data = data_train,
                          kernel = "rbfdot", kpar=list(sigma=0.9),cross=5)

```

#### Prediction and Confusion matrix
```{r}
# predictions on testing dataset
letter_predictions_rbf2 <- predict(letter_classifier_rbf2, data_test)
confusionMatrix(data_test$letter,letter_predictions_rbf2) #Accuracy:0.9422
```

#### 2. Set sigma=0.095 and cross=5 


```{r}
#Gaussian radial basis function rbfdot
letter_classifier_rbf3 <- ksvm(letter ~ ., data = data_train,
                          kernel = "rbfdot", kpar=list(sigma=0.095),cross=5)

```

#### Prediction and Confusion matrix
```{r}
# predictions on testing dataset
letter_predictions_rbf3 <- predict(letter_classifier_rbf3, data_test)
confusionMatrix(data_test$letter,letter_predictions_rbf3) 
```
The model with highest accuracy was the SVM model with radial base function kernal and sigma of 0.09 and having 5-fold cross validation. This model was able to get 95.3% accuracy in predicting the letters.





